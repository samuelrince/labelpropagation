{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://c7bc9d3779d0:4040\n",
       "SparkContext available as 'sc' (version = 2.3.1, master = local[*], app id = local-1618522443627)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-15 21:34:02 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.SparkContext\n",
       "import org.apache.spark.SparkContext._\n",
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.rdd.PairRDDFunctions._\n",
       "import org.apache.spark.SparkContext\n",
       "import org.apache.spark.graphx.Edge\n",
       "import org.apache.spark.graphx.TripletFields\n",
       "import org.apache.spark.graphx.Graph\n",
       "import org.apache.spark.graphx.VertexId\n",
       "import org.apache.spark.graphx.EdgeTriplet\n",
       "import org.apache.spark.graphx.GraphLoader\n",
       "import org.apache.spark._\n",
       "import org.apache.spark.graphx._\n",
       "import scala.util.Random\n",
       "import scala.io.Source\n",
       "import java.io._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.rdd.PairRDDFunctions._\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.graphx.Edge\n",
    "import org.apache.spark.graphx.TripletFields\n",
    "import org.apache.spark.graphx.Graph\n",
    "import org.apache.spark.graphx.VertexId\n",
    "import org.apache.spark.graphx.EdgeTriplet\n",
    "import org.apache.spark.graphx.GraphLoader\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "import scala.util.Random\n",
    "import scala.io.Source\n",
    "import java.io._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class VertexAttr\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VertexAttr(\n",
    "    var degree : Int = 0,\n",
    "    var initialLabel : Boolean = false,\n",
    "    var vertexLabels : Array[Float] = Array[Float](0, 0, 0)\n",
    ") extends java.io.Serializable {     \n",
    "    \n",
    "    def setDegree(degree: Int) = {\n",
    "        this.degree = degree\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readLabel: (filename: String)scala.collection.immutable.Map[Int,Int]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def readLabel(filename: String) = {\n",
    "    val pairs = \n",
    "        for {\n",
    "            line <- Source.fromFile(filename).getLines()\n",
    "            split = line.split(\" \").map(_.trim)\n",
    "            name = split.head.toInt\n",
    "            scores = split.tail(0).toInt\n",
    "        } yield (name -> scores)\n",
    "        pairs.toMap\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initialGraph: org.apache.spark.graphx.Graph[Int,Int] = org.apache.spark.graphx.impl.GraphImpl@24d0133\n",
       "initialLabel: scala.collection.immutable.Map[Int,Int] = Map(341262 -> 13164, 140385 -> 13164, 238478 -> 13428, 255433 -> 13428, 143890 -> 13428, 231297 -> 13428, 31885 -> 13164, 321404 -> 13164, 338021 -> 13164, 164871 -> 13428, 80586 -> 13428, 182424 -> 13428, 259620 -> 13428, 251598 -> 13428, 332709 -> 13428, 168780 -> 13428, 319598 -> 13428, 266519 -> 13428, 204758 -> 13428, 215513 -> 13164, 212578 -> 13164, 190394 -> 13428, 256215 -> 13428, 131212 -> 13164, 139973 -> 13164, 230575 -> 13428, 186425 -> 13428, 250357 -> 13164, 388818 -> 13428, 137573 -> 13428, 325737 -> 13164, 217990 -> 13164, 261943 -> 13164, 128542 -> 13164, 278252 -> 13164, 295703 -> 13164, 107274 -> 13164, 161548 -..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load initial Graph\n",
    "val initialGraph = GraphLoader.edgeListFile(sc, \"./data/com-dblp/com-dblp-ungraph-custom.txt\", canonicalOrientation=false)\n",
    "val initialLabel = readLabel(\"./data/com-dblp/com-dblp-label-custom.txt\")\n",
    "val n_label = initialLabel.map({ case (k, v) => v }).max + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rand: scala.util.Random = scala.util.Random@69ef94d7\n",
       "probability: Double = 0.5\n",
       "random_label: (random: scala.util.Random)Int\n",
       "transform: (VD: org.apache.spark.graphx.VertexId)VertexAttr\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rand = new Random(42)\n",
    "val probability = 0.5\n",
    "\n",
    "def random_label(random: Random): Int = {\n",
    "    random.nextInt(3)\n",
    "}\n",
    "\n",
    "def transform(VD: VertexId): VertexAttr = {\n",
    "    var labelArray = Array.ofDim[Float](n_label)\n",
    "    var va = new VertexAttr(0, false, labelArray);\n",
    "    \n",
    "    if (initialLabel.contains(VD.toInt)) {\n",
    "        var label = initialLabel(VD.toInt)\n",
    "        \n",
    "        if (rand.nextFloat() < probability){\n",
    "            labelArray(label) = 1\n",
    "            va.initialLabel = true\n",
    "            va.vertexLabels = labelArray\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    va;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph: org.apache.spark.graphx.Graph[VertexAttr,Int] = org.apache.spark.graphx.impl.GraphImpl@67798500\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Transform vertices into VertexAttr with a random label assigned\n",
    "val graph = initialGraph.mapVertices({ case (id, attr) => transform(id)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vertex_rdd: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, VertexAttr)] = MapPartitionsRDD[20] at map at <console>:66\n",
       "edge_rdd: org.apache.spark.graphx.EdgeRDD[Int] = EdgeRDDImpl[14] at RDD at EdgeRDD.scala:41\n",
       "graph2: org.apache.spark.graphx.Graph[VertexAttr,Int] = org.apache.spark.graphx.impl.GraphImpl@2162bcdf\n",
       "finalGraph: org.apache.spark.graphx.Graph[VertexAttr,Int] = org.apache.spark.graphx.impl.GraphImpl@54bbb4d\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Populate Graph degree for each vertices and create final graph\n",
    "val vertex_rdd = graph.degrees.zip(graph.vertices).map({ case (vDeg, vAttr) => vAttr._2.setDegree(vDeg._2) ; vAttr})\n",
    "val edge_rdd = graph.edges\n",
    "val graph2 = Graph(vertex_rdd, edge_rdd)\n",
    "\n",
    "// Edges go both sides ways\n",
    "val finalGraph = Graph(graph2.vertices, graph2.edges.union(graph2.edges.reverse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VertexProgram: (id: org.apache.spark.graphx.VertexId, VD: VertexAttr, A: Array[Float])VertexAttr\n",
       "SendMsg: (triplet: org.apache.spark.graphx.EdgeTriplet[VertexAttr,Int])Iterator[(org.apache.spark.graphx.VertexId, Array[Float])]\n",
       "MergeMsg: (A: Array[Float], B: Array[Float])Array[Float]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Vertex Progam\n",
    "def VertexProgram(id : VertexId, \n",
    "                  VD : VertexAttr, \n",
    "                  A : Array[Float]) : VertexAttr  = {\n",
    "    var newVertexAttr = new VertexAttr()\n",
    "    newVertexAttr.degree = VD.degree\n",
    "    newVertexAttr.initialLabel = VD.initialLabel\n",
    "    newVertexAttr.vertexLabels = VD.vertexLabels\n",
    "    \n",
    "    if(VD.initialLabel == false){\n",
    "        newVertexAttr.vertexLabels = A.map(_ / VD.degree)\n",
    "    }\n",
    "    newVertexAttr\n",
    "}\n",
    "\n",
    "// Send Message\n",
    "def SendMsg(triplet : EdgeTriplet[VertexAttr, Int]): Iterator[(VertexId, Array[Float])] = {\n",
    "    Iterator((triplet.dstId, triplet.srcAttr.vertexLabels))\n",
    "}\n",
    "\n",
    "// Merge Message\n",
    "def MergeMsg(A : Array[Float],\n",
    "             B : Array[Float]) : Array[Float] = {\n",
    "    val mergeMsg = new Array[Float](A.length);\n",
    "    for ( i <- 0 to (A.length - 1)) {\n",
    "        mergeMsg(i) = A(i) + B(i);\n",
    "    }\n",
    "    mergeMsg;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// finalGraph.vertices.foreach(v => println(v._1, v._2.degree, v._2.initialLabel, v._2.vertexLabels.mkString(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-15 21:34:22 WARN  BlockManager:66 - Block rdd_11_0 could not be removed as it was not found on disk or in memory\n",
      "2021-04-15 21:34:22 ERROR Executor:91 - Exception in task 0.0 in stage 4.0 (TID 10)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)\n",
      "\tat scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)\n",
      "\tat scala.Array$.ofDim(Array.scala:218)\n",
      "\tat $line10.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.transform(<console>:65)\n",
      "\tat $line11.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:64)\n",
      "\tat $line11.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:64)\n",
      "\tat org.apache.spark.graphx.impl.VertexPartitionBaseOps.map(VertexPartitionBaseOps.scala:61)\n",
      "\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$mapVertices$1.apply(GraphImpl.scala:136)\n",
      "\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$mapVertices$1.apply(GraphImpl.scala:136)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:217)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1092)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.graphx.VertexRDD.compute(VertexRDD.scala:69)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "2021-04-15 21:34:23 ERROR SparkUncaughtExceptionHandler:91 - Uncaught exception in thread Thread[Executor task launch worker for task 10,5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)\n",
      "\tat scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)\n",
      "\tat scala.Array$.ofDim(Array.scala:218)\n",
      "\tat $line10.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.transform(<console>:65)\n",
      "\tat $line11.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:64)\n",
      "\tat $line11.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:64)\n",
      "\tat org.apache.spark.graphx.impl.VertexPartitionBaseOps.map(VertexPartitionBaseOps.scala:61)\n",
      "\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$mapVertices$1.apply(GraphImpl.scala:136)\n",
      "\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$mapVertices$1.apply(GraphImpl.scala:136)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:217)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1092)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.graphx.VertexRDD.compute(VertexRDD.scala:69)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "2021-04-15 21:34:25 WARN  BlockManager:66 - Block rdd_11_1 could not be removed as it was not found on disk or in memory\n",
      "2021-04-15 21:34:25 ERROR Executor:91 - Exception in task 1.0 in stage 4.0 (TID 11)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)\n",
      "\tat scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)\n",
      "\tat scala.Array$.ofDim(Array.scala:218)\n",
      "\tat $line10.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.transform(<console>:65)\n",
      "\tat $line11.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:64)\n",
      "\tat $line11.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:64)\n",
      "\tat org.apache.spark.graphx.impl.VertexPartitionBaseOps.map(VertexPartitionBaseOps.scala:61)\n",
      "\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$mapVertices$1.apply(GraphImpl.scala:136)\n",
      "\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$mapVertices$1.apply(GraphImpl.scala:136)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:217)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1092)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.graphx.VertexRDD.compute(VertexRDD.scala:69)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "2021-04-15 21:34:25 ERROR SparkUncaughtExceptionHandler:91 - [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 11,5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)\n",
      "\tat scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)\n",
      "\tat scala.Array$.ofDim(Array.scala:218)\n",
      "\tat $line10.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.transform(<console>:65)\n",
      "\tat $line11.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:64)\n",
      "\tat $line11.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:64)\n",
      "\tat org.apache.spark.graphx.impl.VertexPartitionBaseOps.map(VertexPartitionBaseOps.scala:61)\n",
      "\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$mapVertices$1.apply(GraphImpl.scala:136)\n",
      "\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$mapVertices$1.apply(GraphImpl.scala:136)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:217)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1092)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.graphx.VertexRDD.compute(VertexRDD.scala:69)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-15 21:34:25 WARN  TaskSetManager:66 - Lost task 0.0 in stage 4.0 (TID 10, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)\n",
      "\tat scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)\n",
      "\tat scala.Array$.ofDim(Array.scala:218)\n",
      "\tat $line10.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.transform(<console>:65)\n",
      "\tat $line11.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:64)\n",
      "\tat $line11.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:64)\n",
      "\tat org.apache.spark.graphx.impl.VertexPartitionBaseOps.map(VertexPartitionBaseOps.scala:61)\n",
      "\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$mapVertices$1.apply(GraphImpl.scala:136)\n",
      "\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$mapVertices$1.apply(GraphImpl.scala:136)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:217)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1092)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.graphx.VertexRDD.compute(VertexRDD.scala:69)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\n",
      "2021-04-15 21:34:25 ERROR TaskSetManager:70 - Task 0 in stage 4.0 failed 1 times; aborting job\n"
     ]
    }
   ],
   "source": [
    "val initialMsg = Array.ofDim[Float](n_label)\n",
    "val maxIter = 2\n",
    "\n",
    "val graphLP = finalGraph.pregel(initialMsg, maxIter, EdgeDirection.Either)(\n",
    "    (id, VD, newLabelsArray) => VertexProgram(id, VD, newLabelsArray),\n",
    "    tr => SendMsg(tr),\n",
    "    (a, b) => MergeMsg(a, b)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graphLP.vertices.foreach(v => println(v._1, v._2.initialLabel, v._2.vertexLabels.mkString(\",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
