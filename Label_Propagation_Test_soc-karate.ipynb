{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://b40790111e29:4040\n",
       "SparkContext available as 'sc' (version = 2.3.1, master = local[*], app id = local-1618501230566)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-15 15:40:29 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.SparkContext\n",
       "import org.apache.spark.graphx.Edge\n",
       "import org.apache.spark.graphx.TripletFields\n",
       "import org.apache.spark.graphx.Graph\n",
       "import org.apache.spark.graphx.VertexId\n",
       "import org.apache.spark.graphx.EdgeTriplet\n",
       "import org.apache.spark.graphx.GraphLoader\n",
       "import org.apache.spark._\n",
       "import org.apache.spark.graphx._\n",
       "import scala.util.Random\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.graphx.Edge\n",
    "import org.apache.spark.graphx.TripletFields\n",
    "import org.apache.spark.graphx.Graph\n",
    "import org.apache.spark.graphx.VertexId\n",
    "import org.apache.spark.graphx.EdgeTriplet\n",
    "import org.apache.spark.graphx.GraphLoader\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "import scala.util.Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class VertexAttr\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VertexAttr(\n",
    "    var degree : Int = 0,\n",
    "    var initialLabel : Boolean = false,\n",
    "    var vertexLabels : Array[Float] = Array[Float](0, 0, 0)\n",
    ") extends java.io.Serializable {     \n",
    "    \n",
    "    def setDegree(degree: Int) = {\n",
    "        this.degree = degree\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rand: scala.util.Random = scala.util.Random@18fb517b\n",
       "random_label: (random: scala.util.Random)Int\n",
       "transform: (VD: Int)VertexAttr\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rand = new Random(42)\n",
    "\n",
    "def random_label(random: Random): Int = {\n",
    "    random.nextInt(3)\n",
    "}\n",
    "\n",
    "def transform(VD: Int): VertexAttr = {\n",
    "    val label = random_label(rand);\n",
    "    var va = new VertexAttr();\n",
    "    \n",
    "    if (rand.nextInt(2) == 1) {\n",
    "        if (label == 0) {\n",
    "            va.initialLabel = true;\n",
    "            va.vertexLabels = Array[Float](1, 0, 0);\n",
    "        } else if (label == 1) {\n",
    "            va.initialLabel = true;\n",
    "            va.vertexLabels = Array[Float](0, 1, 0);\n",
    "        } else if (label == 2) {\n",
    "            va.initialLabel = true;\n",
    "            va.vertexLabels = Array[Float](0, 0, 1);\n",
    "        }\n",
    "    }\n",
    "    va;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initialGraph: org.apache.spark.graphx.Graph[Int,Int] = org.apache.spark.graphx.impl.GraphImpl@6e5b003b\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load initial Graph\n",
    "val initialGraph = GraphLoader.edgeListFile(sc, \"./soc-karate/soc-karate.mtx\", canonicalOrientation=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph: org.apache.spark.graphx.Graph[VertexAttr,Int] = org.apache.spark.graphx.impl.GraphImpl@5b460bfb\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Transform vertices into VertexAttr with a random label assigned\n",
    "val graph = initialGraph.mapVertices({ case (id, attr) => transform((attr))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vertex_rdd: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, VertexAttr)] = MapPartitionsRDD[20] at map at <console>:51\n",
       "edge_rdd: org.apache.spark.graphx.EdgeRDD[Int] = EdgeRDDImpl[14] at RDD at EdgeRDD.scala:41\n",
       "graph2: org.apache.spark.graphx.Graph[VertexAttr,Int] = org.apache.spark.graphx.impl.GraphImpl@3e7e2058\n",
       "finalGraph: org.apache.spark.graphx.Graph[VertexAttr,Int] = org.apache.spark.graphx.impl.GraphImpl@679b4bc2\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Populate Graph degree for each vertices and create final graph\n",
    "val vertex_rdd = graph.degrees.zip(graph.vertices).map({ case (vDeg, vAttr) => vAttr._2.setDegree(vDeg._2) ; vAttr})\n",
    "val edge_rdd = graph.edges\n",
    "\n",
    "val graph2 = Graph(vertex_rdd, edge_rdd)\n",
    "\n",
    "// Edges go both sides ways\n",
    "val finalGraph = Graph(graph2.vertices, graph2.edges.union(graph2.edges.reverse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VertexProgram: (id: org.apache.spark.graphx.VertexId, VD: VertexAttr, A: Array[Float])VertexAttr\n",
       "SendMsg: (triplet: org.apache.spark.graphx.EdgeTriplet[VertexAttr,Int])Iterator[(org.apache.spark.graphx.VertexId, Array[Float])]\n",
       "MergeMsg: (A: Array[Float], B: Array[Float])Array[Float]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Vertex Progam\n",
    "def VertexProgram(id : VertexId, \n",
    "                  VD : VertexAttr, \n",
    "                  A : Array[Float]) : VertexAttr  = {\n",
    "    var newVertexAttr = new VertexAttr()\n",
    "    newVertexAttr.degree = VD.degree\n",
    "    newVertexAttr.initialLabel = VD.initialLabel\n",
    "    newVertexAttr.vertexLabels = VD.vertexLabels\n",
    "    \n",
    "    if(VD.initialLabel == false){\n",
    "        newVertexAttr.vertexLabels = A.map(_ / VD.degree)\n",
    "    }\n",
    "    newVertexAttr\n",
    "}\n",
    "\n",
    "// Send Message\n",
    "def SendMsg(triplet : EdgeTriplet[VertexAttr, Int]): Iterator[(VertexId, Array[Float])] = {\n",
    "//     println(triplet.srcId, \"->\", triplet.dstId, triplet.srcAttr.vertexLabels.mkString(\", \"))\n",
    "    Iterator((triplet.dstId, triplet.srcAttr.vertexLabels))\n",
    "}\n",
    "\n",
    "// Merge Message\n",
    "def MergeMsg(A : Array[Float],\n",
    "             B : Array[Float]) : Array[Float] = {\n",
    "    val mergeMsg = new Array[Float](A.length);\n",
    "    for ( i <- 0 to (A.length - 1)) {\n",
    "         mergeMsg(i) = A(i) + B(i);\n",
    "    }\n",
    "    mergeMsg;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initialMsg: Array[Float] = Array(0.0, 0.0, 0.0)\n",
       "maxIter: Int = 100\n",
       "graphLP: org.apache.spark.graphx.Graph[VertexAttr,Int] = org.apache.spark.graphx.impl.GraphImpl@20bb4c03\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val initialMsg = Array[Float](0, 0, 0)\n",
    "val maxIter = 100\n",
    "\n",
    "val graphLP = finalGraph.pregel(initialMsg, maxIter, EdgeDirection.Either)(\n",
    "    (id, VD, newLabelsArray) => VertexProgram(id, VD, newLabelsArray),\n",
    "    tr => SendMsg(tr),\n",
    "    (a, b) => MergeMsg(a, b)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13,2,false,0.058400773 0.30003047 0.64156866)\n",
      "(19,2,false,0.7078702 0.08155619 0.21057361)\n",
      "(15,2,true,1.0 0.0 0.0)\n",
      "(21,2,true,0.0 0.0 1.0)\n",
      "(25,3,false,0.2843869 0.016873697 0.69873935)\n",
      "(29,3,false,0.13858013 0.054370794 0.8070491)\n",
      "(11,3,false,0.027413137 0.6894467 0.28314006)\n",
      "(27,2,true,0.0 0.0 1.0)\n",
      "(33,12,true,1.0 0.0 0.0)\n",
      "(23,2,true,0.0 1.0 0.0)\n",
      "(1,16,false,0.054826275 0.37889344 0.5662801)\n",
      "(17,2,false,0.0 1.0 0.0)\n",
      "(3,10,true,0.0 0.0 1.0)\n",
      "(7,4,true,0.0 1.0 0.0)\n",
      "(9,5,false,0.38856718 0.13522969 0.47620305)\n",
      "(31,4,false,0.47226927 0.13414264 0.39358807)\n",
      "(5,3,false,0.027413137 0.6894467 0.28314006)\n",
      "(34,17,false,0.41574037 0.16311239 0.42114723)\n",
      "(4,6,false,0.061975274 0.2211675 0.71685714)\n",
      "(16,2,true,1.0 0.0 0.0)\n",
      "(22,2,true,0.0 0.0 1.0)\n",
      "(28,4,false,0.4250318 0.044996522 0.52997166)\n",
      "(30,4,false,0.6039351 0.040778097 0.3552868)\n",
      "(14,5,false,0.1234623 0.20028035 0.6762573)\n",
      "(32,6,true,0.0 0.0 1.0)\n",
      "(24,5,true,1.0 0.0 0.0)\n",
      "(6,4,true,0.0 1.0 0.0)\n",
      "(8,4,false,0.05039277 0.20957235 0.7400348)\n",
      "(12,1,false,0.054826275 0.37889344 0.5662801)\n",
      "(18,2,true,0.0 0.0 1.0)\n",
      "(20,3,true,0.0 1.0 0.0)\n",
      "(26,3,false,0.42812896 0.0056245658 0.56624645)\n",
      "(10,2,false,0.20787019 0.08155619 0.7105736)\n",
      "(2,9,false,0.08476954 0.23822846 0.67700195)\n"
     ]
    }
   ],
   "source": [
    "graphLP.vertices.foreach(v => println(v._1, v._2.degree, v._2.initialLabel, v._2.vertexLabels.mkString(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
